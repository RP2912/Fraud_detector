{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acde4e10",
   "metadata": {},
   "source": [
    "## ðŸ§ª EDA - Fraud Detection Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38b9eeba",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "# import seaborn as sns\n",
    "# from ydata_profiling import ProfileReport\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "\n",
    "# Profiling using a sample of the data can indeed provide a clear picture of the overall dataset, especially when working with very large datasets. Therefore to get the understanding about the data, we'll do profiling\n",
    "\n",
    "# %%\n",
    "# Sample a fraction of the data (e.g., 1% of the data)\n",
    "\n",
    "df = pd.read_csv(r'D:\\Data science\\Projects\\Fraud_detection\\fraud_detection_app\\data\\Fraud.csv', low_memory=False)\n",
    "\n",
    "\n",
    "# Generate the Pandas Profiling report\n",
    "# profile = ProfileReport(sample_df, title=\"Pandas Profiling Report\", explorative=True)\n",
    "\n",
    "# # Save the report as an HTML file\n",
    "# profile.to_file(\"profiling_report.html\")\n",
    "\n",
    "# # %%\n",
    "# profile\n",
    "\n",
    "# # %%\n",
    "# Insights:\n",
    "\n",
    "# 1.isFlaggedFraud:\n",
    "# Constant Value: This column has a constant value of \"0\".\n",
    "\n",
    "# 2.nameOrig:\n",
    "# High Cardinality: Contains 63,625 distinct values.\n",
    "# Uniform Distribution: The values are uniformly distributed.\n",
    "# Unique Values: Each value is unique.\n",
    "\n",
    "# 3.nameDest:\n",
    "# High Cardinality: Contains 60,682 distinct values.\n",
    "# Uniform Distribution: The values are uniformly distributed.\n",
    "\n",
    "# 4.isFraud:\n",
    "# Highly Imbalanced: The dataset is highly imbalanced with only 1.4% of the transactions being fraudulent (98.6% non-fraudulent).\n",
    "\n",
    "# 5.amount:\n",
    "# Highly Skewed: Skewness of 30.165, indicating a highly skewed distribution.\n",
    "\n",
    "# 6.oldbalanceOrg:\n",
    "# Zeros: Contains 20,917 zeros (32.9% of the values).\n",
    "\n",
    "# 7.newbalanceOrig:\n",
    "# Zeros: Contains 35,926 zeros (56.5% of the values).\n",
    "\n",
    "# 8.oldbalanceDest:\n",
    "# Highly Skewed: Skewness of 26.748, indicating a highly skewed distribution.\n",
    "# Zeros: Contains 27,057 zeros (42.5% of the values).\n",
    "\n",
    "# 9.newbalanceDest:\n",
    "# Highly Skewed: Skewness of 24.522, indicating a highly skewed distribution.\n",
    "# Zeros: Contains 24,403 zeros (38.4% of the values).\n",
    "\n",
    "# %%\n",
    "# Check for missing values for the whole data\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# %% [markdown]\n",
    "# ## No null values found\n",
    "\n",
    "# %%\n",
    "# We can now get the deep insight of whole data by performing EDA\n",
    "\n",
    "# %% [markdown]\n",
    "# # EDA\n",
    "\n",
    "# %%\n",
    "# Correlation matrix\n",
    "corr_matrix = df.corr()\n",
    "\n",
    "# Heatmap of the correlation matrix\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "# plt.show()\n",
    "\n",
    "# The correlation matrix shows a multi-collinearity between oldbalanceDest and newbalanceDest & newbalanceOrig and oldbalanceOrig (correlation coefficient of 0.98, 1). This indicates that these features are highly redundant, and keeping them may not add much value to our model\n",
    "# Highly correlated features can cause multicollinearity in regression models, which can make the model coefficients unstable and harder to interpret\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n",
    "# # Drop the highly correlated feat\n",
    "df.drop(columns=['oldbalanceDest','oldbalanceOrg'],axis=1,inplace=True)\n",
    "# # to improve the model performance\n",
    "\n",
    "# %%\n",
    "df.head()\n",
    "\n",
    "# %%\n",
    "# Data Distribution Visualization\n",
    "df.hist(figsize=(20, 15))\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# STEP: The data is spread across the entire range of steps, with more transactions occurring around the middle steps (100 to 300 hours). This indicates a relatively even distribution of transactions over time, with some variation.\n",
    "#\n",
    "# AMOUNT: The majority of transaction amounts are clustered towards the lower end of the scale, with a long tail extending towards very high amounts. This indicates a positively skewed distribution with a few large transactions.\n",
    "#\n",
    "# newBALANCEORG: Most initial balances are very low, with a few accounts having significantly higher balances. This shows a skewed distribution with many small values and a long tail of larger values. Same for newbalanceDest.\n",
    "#\n",
    "# isFraud:The histogram shows a highly imbalanced dataset with the majority of transactions being non-fraudulent. Very few transactions are marked as fraudulent. Same for isFlaggedFraud.\n",
    "\n",
    "# %%\n",
    "plt.figure(figsize=(20, 15))\n",
    "sns.boxplot(data=df)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# here, we can two problems\n",
    "# 1. The data is nt scaled for whcich we can either use minmax scaler, normalization, clipping, log transformation, binning.\n",
    "# 2. There are outliers which we can remove as possibility of fraud can be there in such case.\n",
    "#\n",
    "# Considering both the issue to resolve, and I dont want to remove outliers and scaling needs to be done. So, clipping preferable.\n",
    "\n",
    "# %%\n",
    "df.head()\n",
    "\n",
    "# %%\n",
    "df['Amount_Clip'] = df['amount'].clip(lower=0, upper=1000000)\n",
    "df['NewBalOrig_Clip'] = df['newbalanceOrig'].clip(lower=0, upper=1000000)\n",
    "df['BalDest_Clip'] = df['newbalanceDest'].clip(lower=0, upper=1000000)\n",
    "\n",
    "# %%\n",
    "upper_threshold1 = df['amount'].quantile(0.99)\n",
    "upper_threshold2= df['newbalanceOrig'].quantile(0.99)\n",
    "upper_threshold3= df['newbalanceDest'].quantile(0.99)\n",
    "upper_threshold1,upper_threshold2,upper_threshold3\n",
    "\n",
    "# %% [markdown]\n",
    "# Not considering clipping due to lack of clarity which can gain with the discussion with client. Instead of clipping using Robustscaler\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "# %%\n",
    "features_to_scale = ['amount', 'newbalanceOrig', 'newbalanceDest']\n",
    "data_to_scale = df[features_to_scale].values\n",
    "scaler = RobustScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "\n",
    "\n",
    "# %%\n",
    "scaled_data\n",
    "\n",
    "# %%\n",
    "# Replace original columns with scaled data\n",
    "df[features_to_scale] = scaled_data\n",
    "\n",
    "\n",
    "# %%\n",
    "df.head()\n",
    "\n",
    "# %% [markdown]\n",
    "# Also, from profiling we come to know nameDest, nameOrig has high cardinality, and each value is unique. Clearly, keeping such value would effect the performance. Therefore dropping both the features\n",
    "\n",
    "# %%\n",
    "df.drop(['nameOrig','nameDest'],inplace=True,axis=1)\n",
    "\n",
    "# %%\n",
    "df['step'].unique()\n",
    "\n",
    "# %%\n",
    "df['step'].unique().size\n",
    "\n",
    "# %% [markdown]\n",
    "# Here,analysis does not require understanding the timing of transactions (e.g., when they occur within the 30-day period), the step column might not add value.\n",
    "\n",
    "# %%\n",
    "df.drop('step',axis=1,inplace=True)\n",
    "\n",
    "# %%\n",
    "df.head()\n",
    "\n",
    "# %%\n",
    "df['type'].unique()\n",
    "\n",
    "# %%\n",
    "# Feature encoding\n",
    "\n",
    "# %%\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Apply LabelEncoder to 'transaction_type' column\n",
    "df['transaction_type'] = label_encoder.fit_transform(df['type'])\n",
    "df.head()\n",
    "\n",
    "# %%\n",
    "\n",
    "df.drop('type',axis=1,inplace=True)\n",
    "\n",
    "# %%\n",
    "df.head()\n",
    "\n",
    "# %%\n",
    "\n",
    "# Create count plot\n",
    "# plt.figure(figsize=(6, 4))\n",
    "# sns.countplot(x='isFlaggedFraud', data=df, palette='viridis')\n",
    "\n",
    "# # Add titles and labels\n",
    "# plt.title('Count of Fraud Flagged Transactions')\n",
    "# plt.xlabel('Is Flagged Fraud')\n",
    "# plt.ylabel('Count')\n",
    "# plt.xticks(ticks=[0, 1], labels=['Not Flagged', 'Flagged'])\n",
    "\n",
    "# # %%\n",
    "# cor=df['isFlaggedFraud'].corr(df['isFraud'])\n",
    "# cor\n",
    "\n",
    "# %% [markdown]\n",
    "# this data is completely imbalance, and its impact on target variable is also very low, we may prefer to remove it\n",
    "\n",
    "# %%\n",
    "\n",
    "# # Create count plot\n",
    "# plt.figure(figsize=(6, 4))\n",
    "# sns.countplot(x='isFraud', data=df, palette='viridis')\n",
    "\n",
    "# %% [markdown]\n",
    "# TARGET variablle is imbalanceif would use stratify while model training, depending on the performance we may opt SMOTE further\n",
    "\n",
    "# %%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
